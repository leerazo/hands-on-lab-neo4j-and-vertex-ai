{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d8979e",
   "metadata": {},
   "source": [
    "# Chatbot\n",
    "In this notebook, we'll build a chatbot.  The chatbot interface will use gradio.  Underlying the chatbot is the Neo4j knowledge graph we built in previous labs.  The chatbot uses generative AI and langchain.  We take a natural language question, convert it to Neo4j Cypher using generative AI and run that against the database.  We take the response from the database and use generative AI again to convert that back to natural language before presenting it to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea81aba-c3e5-435b-9a9a-86d0a65da844",
   "metadata": {},
   "source": [
    "## Base Example Without Grounding\n",
    "Before grounding with the Neo4j, let's setup up a baseline that just uses an LLM to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d45b7-1406-4454-8811-67a2b351c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import VertexAI\n",
    "\n",
    "base_chain = VertexAI(model_name='text-bison', max_output_tokens=2048, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6d342-e9dc-4533-955e-601e181011ae",
   "metadata": {},
   "source": [
    "We can now ask a simple finance question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f933a2d9-53fd-41df-8b5a-248885b0dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"What are the top 10 investments for Rempart?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f64a21-8347-4eec-a83a-8044f824a3d8",
   "metadata": {},
   "source": [
    "While this answer looks reasonable, we have no real way to know how the LLM came it with it, or where it was sourced from.\n",
    "\n",
    "Here is a more complicated example where we expect the LLM to understand some more specific terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43c396-39d5-4511-bc3d-a56a1880afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = base_chain(\"\"\"Which managers own FAANG stocks?\"\"\")\n",
    "print(f\"Final answer: {base_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4206b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import typing_extensions\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\", return_messages = True)\n",
    "agent_chain = base_chain\n",
    "\n",
    "def chat_response(input_text,history):\n",
    "\n",
    "    try:\n",
    "        return agent_chain(input_text)\n",
    "    except:\n",
    "        # a bit of protection against exposed error messages\n",
    "        # we could log these situations in the backend to revisit later in development\n",
    "        return \"I'm sorry, there was an error retrieving the information you requested.\"\n",
    "\n",
    "interface = gr.ChatInterface(fn = chat_response,\n",
    "                             title = \"Ungrounded Chatbot\",\n",
    "                             description = \"NOT powered by Neo4j\",\n",
    "                             theme = \"soft\",\n",
    "                             chatbot = gr.Chatbot(height=500),\n",
    "                             undo_btn = None,\n",
    "                             clear_btn = \"\\U0001F5D1 Clear chat\",\n",
    "                             examples = [\"What are the top 10 investments for Rempart?\",\n",
    "                                         \"Which manager owns FAANG stocks?\",\n",
    "                                         \"What are other top investments for fund managers investing in Exxon?\",\n",
    "                                         \"What are Rempart's top investments by value for 2023?\",\n",
    "                                         \"Who are the common investors between Tesla and Costco?\",\n",
    "                                         \"Who are Tesla's top investors in last 3 months?\"])\n",
    "\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00e27bc-e586-467b-8ace-6d3893d2fad1",
   "metadata": {},
   "source": [
    "## Fine Tuning for Cypher Generation\n",
    "We encourage you to use Vertex AI Codey family models with a schema, few-shot examples, and precise prompt engineering for Cypher generation. However, if that still doesn't provide an appropriate level of quality, or you need your LLM to improve accuracy on a more specific task area, you can try fine-tuning.\n",
    "\n",
    "Fine-tuning is the process of taking a foundational model and making precise changes to improve its performance for a specific, narrower task. It works by taking in training data containing many examples of your specific task and using it to update or add additional parameters in a new version of the model.\n",
    "\n",
    "The total training time generally takes more than an hour. The tuned adapter model is going to stay within your tenant, and your training data will not be used to train the base model, which is frozen. Tuning runs on GCP's TPU infrastructure that is optimized to run ML workloads.\n",
    "\n",
    "The training data should be structured as a supervised training set, where each row contains input text and desired, resulting, Cypher query. Vertex AI expects you to adhere to the below format in a `jsonl` file.\n",
    "\n",
    "```\n",
    "{\"input_text\": \"MY_INPUT_PROMPT\", \"output_text\": \"CYPHER_QUERY\"}\n",
    "```\n",
    "You can find more about fine-tuning in the [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5197afd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we went through the steps of connecting a LangChain agent to a Neo4j database and using it to generate Cypher queries in response to user requests via LLMs on Vertex AI, thus grounding the LLM with a knowledge graph.\n",
    "\n",
    "While we used the `code-bison` model here, this approach can be generalized to other Vertex AI LLMs.  This process can also be augmented with additional steps around the generation chain to customize the experience for specific use cases.  \n",
    "\n",
    "The critical takeaway is the importance of Neo4j Knowledge Graph as a grounding database to: \n",
    "* Anchor your chatbot to reality as it generates responses and \n",
    "* Enable your LLM to provide answers enriched with relevant enterprise data."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
